{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0419 01:33:36.786133 139977744742208 <ipython-input-10-b35dc5619532>:154] Reading from Pet dataset.\n",
      "I0419 01:33:36.788373 139977744742208 <ipython-input-10-b35dc5619532>:168] 191 training and 11 validation examples.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "r\"\"\"Convert the Oxford pet dataset to TFRecord for object_detection.\n",
    "See: O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar\n",
    "     Cats and Dogs\n",
    "     IEEE Conference on Computer Vision and Pattern Recognition, 2012\n",
    "     http://www.robots.ox.ac.uk/~vgg/data/pets/\n",
    "Example usage:\n",
    "    ./create_pet_tf_record --data_dir=/home/user/pet \\\n",
    "        --output_dir=/home/user/pet/output\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from lxml import etree\n",
    "import PIL.Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# count = 0\n",
    "\n",
    "def dict_to_tf_example(data,\n",
    "                       label_map_dict,\n",
    "                       image_subdirectory,\n",
    "                       ignore_difficult_instances=False):\n",
    "  \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "  Notice that this function normalizes the bounding box coordinates provided\n",
    "  by the raw data.\n",
    "  Args:\n",
    "    data: dict holding PASCAL XML fields for a single image (obtained by\n",
    "      running dataset_util.recursive_parse_xml_to_dict)\n",
    "    label_map_dict: A map from string label names to integers ids.\n",
    "    image_subdirectory: String specifying subdirectory within the\n",
    "      Pascal dataset directory holding the actual image data.\n",
    "    ignore_difficult_instances: Whether to skip difficult instances in the\n",
    "      dataset  (default: False).\n",
    "  Returns:\n",
    "    example: The converted tf.Example.\n",
    "  Raises:\n",
    "    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "  \"\"\"\n",
    "  img_path = os.path.join(image_subdirectory, data['filename'])\n",
    "  with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "    encoded_jpg = fid.read()\n",
    "  encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "  image = PIL.Image.open(encoded_jpg_io)\n",
    "  if image.format != 'JPEG':\n",
    "    raise ValueError('Image format not JPEG')\n",
    "  key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "  width = int(data['size']['width'])\n",
    "  height = int(data['size']['height'])\n",
    "\n",
    "  xmin = []\n",
    "  ymin = []\n",
    "  xmax = []\n",
    "  ymax = []\n",
    "  classes = []\n",
    "  classes_text = []\n",
    "  truncated = []\n",
    "  poses = []\n",
    "  difficult_obj = []\n",
    "  for obj in data['object']:\n",
    "    difficult_obj.append(int(0))\n",
    "\n",
    "    xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "    ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "    xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "    ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "\n",
    "    class_name = obj['name']\n",
    "    classes_text.append(class_name.encode('utf8'))\n",
    "    classes.append(label_map_dict[class_name])\n",
    "    truncated.append(int(0))\n",
    "    poses.append('Unspecified'.encode('utf8'))\n",
    "\n",
    "  example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "      'image/source_id': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "      'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "      'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "\n",
    "def create_tf_record(output_filename,\n",
    "                     label_map_dict,\n",
    "                     annotations_dir,\n",
    "                     image_dir,\n",
    "                     examples):\n",
    "  \"\"\"Creates a TFRecord file from examples.\n",
    "  Args:\n",
    "    output_filename: Path to where output file is saved.\n",
    "    label_map_dict: The label map dictionary.\n",
    "    annotations_dir: Directory where annotation files are stored.\n",
    "    image_dir: Directory where image files are stored.\n",
    "    examples: Examples to parse and save to tf record.\n",
    "  \"\"\"\n",
    "  writer = tf.python_io.TFRecordWriter(output_filename)\n",
    "  for idx, example in enumerate(examples):\n",
    "    path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "      continue\n",
    "    with tf.gfile.GFile(path, 'r') as fid:\n",
    "      xml_str = fid.read()\n",
    "    xml = etree.fromstring(xml_str)\n",
    "    data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "    tf_example = dict_to_tf_example(data, label_map_dict, image_dir)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  writer.close()\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  label_map_dict = label_map_util.get_label_map_dict('annotations/label_map.pbtxt')\n",
    "\n",
    "  logging.info('Reading from Pet dataset.')\n",
    "  image_dir = 'images/'\n",
    "  annotations_dir = 'annotations'\n",
    "  examples_path = os.path.join(annotations_dir, 'trainval.txt')\n",
    "  examples_list = dataset_util.read_examples_list(examples_path)\n",
    "\n",
    "  # Test images are not included in the downloaded data set, so we shall perform\n",
    "  # our own split.\n",
    "  random.seed(42)\n",
    "  random.shuffle(examples_list)\n",
    "  num_examples = len(examples_list)\n",
    "  num_train = int(0.95 * num_examples)\n",
    "  train_examples = examples_list[:num_train]\n",
    "  val_examples = examples_list[num_train:]\n",
    "  logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n",
    "\n",
    "  train_output_path = 'data/train.record'\n",
    "  val_output_path = 'data/test.record'\n",
    "  create_tf_record(train_output_path, label_map_dict, annotations_dir,\n",
    "                   image_dir, train_examples)\n",
    "  create_tf_record(val_output_path, label_map_dict, annotations_dir,\n",
    "                   image_dir, val_examples)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
